{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install accelerate peft bitsandbytes transformers trl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-10T19:41:38.141972Z","iopub.execute_input":"2023-10-10T19:41:38.142709Z","iopub.status.idle":"2023-10-10T19:41:52.684226Z","shell.execute_reply.started":"2023-10-10T19:41:38.142678Z","shell.execute_reply":"2023-10-10T19:41:52.683038Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2023-10-10T19:41:52.686675Z","iopub.execute_input":"2023-10-10T19:41:52.687314Z","iopub.status.idle":"2023-10-10T19:42:07.562173Z","shell.execute_reply.started":"2023-10-10T19:41:52.687277Z","shell.execute_reply":"2023-10-10T19:42:07.561199Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model Config","metadata":{}},{"cell_type":"code","source":"# Local directory for Meta's Llama 2 model\nmodel_name = \"/kaggle/input/llama2-7b-hf/Llama2-7b-hf\"\n\n# New instruction dataset\ndataset_name = \"mlabonne/guanaco-llama2-1k\"\n\n# Fine-tuned model\nnew_model = \"llama-2-7b-chat-guanaco\"","metadata":{"execution":{"iopub.status.busy":"2023-10-10T19:42:07.563452Z","iopub.execute_input":"2023-10-10T19:42:07.563803Z","iopub.status.idle":"2023-10-10T19:42:07.570191Z","shell.execute_reply.started":"2023-10-10T19:42:07.563768Z","shell.execute_reply":"2023-10-10T19:42:07.568694Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Load dataset (you can process it here)\ndataset = load_dataset(dataset_name, split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2023-10-10T19:42:07.572791Z","iopub.execute_input":"2023-10-10T19:42:07.573702Z","iopub.status.idle":"2023-10-10T19:42:09.074867Z","shell.execute_reply.started":"2023-10-10T19:42:07.573672Z","shell.execute_reply":"2023-10-10T19:42:09.073864Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset parquet/mlabonne--guanaco-llama2-1k to /root/.cache/huggingface/datasets/parquet/mlabonne--guanaco-llama2-1k-f1f1134768f90029/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2d92d7a811d4d639d774f5a213c7d9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/967k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8377b3da7f4e4d7dae6839a3821fc561"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcfa2041d04f4c0dbca23e99b40344cb"}},"metadata":{}},{"name":"stdout","text":"Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/mlabonne--guanaco-llama2-1k-f1f1134768f90029/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, \"float16\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T19:42:09.076316Z","iopub.execute_input":"2023-10-10T19:42:09.076850Z","iopub.status.idle":"2023-10-10T19:42:09.083120Z","shell.execute_reply.started":"2023-10-10T19:42:09.076818Z","shell.execute_reply":"2023-10-10T19:42:09.082225Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map={\"\": 0}\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"execution":{"iopub.status.busy":"2023-10-10T19:42:09.084437Z","iopub.execute_input":"2023-10-10T19:42:09.084971Z","iopub.status.idle":"2023-10-10T19:58:25.508847Z","shell.execute_reply.started":"2023-10-10T19:42:09.084941Z","shell.execute_reply":"2023-10-10T19:58:25.507941Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b16fb9026562494496e42522d2e8fc15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9b3905f103f484b92ad9cdcff3cc5b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e30fef148e3446dd9e3c867d19914e0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3766784d29f490696867e60ac3f89c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d366348bf82a4ea5afd1471b0a319be9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc891dc461a14de7b850a60bc2436a4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"925152b81a1043e5a66fa492e86428fd"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2023-10-10T19:58:25.510181Z","iopub.execute_input":"2023-10-10T19:58:25.510998Z","iopub.status.idle":"2023-10-10T19:58:26.888146Z","shell.execute_reply.started":"2023-10-10T19:58:25.510965Z","shell.execute_reply":"2023-10-10T19:58:26.887188Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"576d5b5eca7340328023b9f4273aeba0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b133c5baf354d53a01e4e6f67ba7f19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c43f6de836104463a72cadaac5bc35b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)in/added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e5f76f70b124c21b29d34354e30235e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e639e53b2f3e4e26b74e3b7ee4ada86e"}},"metadata":{}}]},{"cell_type":"code","source":"# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T19:58:26.889591Z","iopub.execute_input":"2023-10-10T19:58:26.890214Z","iopub.status.idle":"2023-10-10T19:58:26.894971Z","shell.execute_reply.started":"2023-10-10T19:58:26.890182Z","shell.execute_reply":"2023-10-10T19:58:26.894038Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=25,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"tensorboard\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T19:58:26.896094Z","iopub.execute_input":"2023-10-10T19:58:26.896624Z","iopub.status.idle":"2023-10-10T19:58:26.910776Z","shell.execute_reply.started":"2023-10-10T19:58:26.896595Z","shell.execute_reply":"2023-10-10T19:58:26.909934Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=None,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T19:58:26.914041Z","iopub.execute_input":"2023-10-10T19:58:26.914277Z","iopub.status.idle":"2023-10-10T19:58:38.280750Z","shell.execute_reply.started":"2023-10-10T19:58:26.914258Z","shell.execute_reply":"2023-10-10T19:58:38.279749Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:166: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4e6d4e22b2848b293968119bb422bd9"}},"metadata":{}}]},{"cell_type":"code","source":"# Train model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-10-10T19:58:38.281989Z","iopub.execute_input":"2023-10-10T19:58:38.282874Z","iopub.status.idle":"2023-10-10T20:41:54.384579Z","shell.execute_reply.started":"2023-10-10T19:58:38.282841Z","shell.execute_reply":"2023-10-10T20:41:54.383047Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 42:50, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.346600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.613300</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.208800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.437900</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.175600</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.360300</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.171800</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.458000</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.154300</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.529700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=250, training_loss=1.3456452026367187, metrics={'train_runtime': 2595.4241, 'train_samples_per_second': 0.385, 'train_steps_per_second': 0.096, 'total_flos': 8755214190673920.0, 'train_loss': 1.3456452026367187, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Save trained model\ntrainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T20:41:54.389382Z","iopub.execute_input":"2023-10-10T20:41:54.390448Z","iopub.status.idle":"2023-10-10T20:41:54.653882Z","shell.execute_reply.started":"2023-10-10T20:41:54.390409Z","shell.execute_reply":"2023-10-10T20:41:54.652853Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# from tensorboard import notebook\n# log_dir = \"/kaggle/working/results/runs\"\n\n# notebook.start(\"--logdir {} --port 4000\".format(log_dir))","metadata":{"execution":{"iopub.status.busy":"2023-10-10T20:41:54.655236Z","iopub.execute_input":"2023-10-10T20:41:54.656133Z","iopub.status.idle":"2023-10-10T20:41:54.660340Z","shell.execute_reply.started":"2023-10-10T20:41:54.656102Z","shell.execute_reply":"2023-10-10T20:41:54.659540Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ignore warnings\nlogging.set_verbosity(logging.CRITICAL)\n\n# Run text generation pipeline with our next model\nprompt = \"Who is Leonardo Da Vinci?\"\npipe = pipeline(task=\"text-generation\", model=new_model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2023-10-10T20:53:18.930848Z","iopub.execute_input":"2023-10-10T20:53:18.931232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"What is Datacamp Career track?\"\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")","metadata":{"execution":{"iopub.status.busy":"2023-10-10T20:50:40.831031Z","iopub.execute_input":"2023-10-10T20:50:40.831369Z","iopub.status.idle":"2023-10-10T20:50:41.109445Z","shell.execute_reply.started":"2023-10-10T20:50:40.831341Z","shell.execute_reply":"2023-10-10T20:50:41.108539Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token $secret_value_0","metadata":{"execution":{"iopub.status.busy":"2023-10-10T20:50:43.640776Z","iopub.execute_input":"2023-10-10T20:50:43.641268Z","iopub.status.idle":"2023-10-10T20:50:45.135971Z","shell.execute_reply.started":"2023-10-10T20:50:43.641234Z","shell.execute_reply":"2023-10-10T20:50:45.134848Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nToken will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.model.push_to_hub(new_model, use_temp_dir=False)\ntrainer.tokenizer.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T20:51:39.344722Z","iopub.execute_input":"2023-10-10T20:51:39.345098Z","iopub.status.idle":"2023-10-10T20:51:48.054330Z","shell.execute_reply.started":"2023-10-10T20:51:39.345071Z","shell.execute_reply":"2023-10-10T20:51:48.053182Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f7ad0b97b0147338942a38cfed8da4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35efe79e46ae48e0892b9b85c32b7fd2"}},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/kingabzpro/llama-2-7b-chat-guanaco/commit/60a4a926d2490bd2b242f11b42b2d694d42a4fe8', commit_message='Upload tokenizer', commit_description='', oid='60a4a926d2490bd2b242f11b42b2d694d42a4fe8', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]}]}